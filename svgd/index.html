<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><link href=/favicon.svg rel=icon type=image/svg+xml><link rel="alternate icon" href=/favicon.png type=image/png><link href=/css/katex.min.css rel=stylesheet><script defer src=/js/katex.min.js></script><script defer onload=renderMathInElement(document.body); src=/js/auto-render.min.js></script><script src=/js/vega.min.js></script><script src=/js/vega-lite.min.js></script><script src=/js/vega-embed.min.js></script><link href=/css/default.css rel=stylesheet><script async crossorigin data-category=Comments data-category-id=DIC_kwDOGkX-x84CAY8r data-emit-metadata=0 data-input-position=top data-lang=en data-mapping=pathname data-reactions-enabled=1 data-repo=raifthenerd/raifthenerd.github.io data-repo-id=R_kgDOGkX-xw data-theme=light src=https://giscus.app/client.js></script><script src="https://www.googletagmanager.com/gtag/js?id=G-XXY96K3XP3" async></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date());gtag('config','G-XXY96K3XP3')</script><script async data-ad-client=ca-pub-5141260618801038 src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><link href=https://blog.raifthenerd.com/atom.xml rel=alternate title=Feeds type=application/atom+xml><meta content=website property=og:type><meta content=https://blog.raifthenerd.com/svgd/ property=og:url><meta content=.sh property=og:site_name><meta content=summary name=twitter:card><title>Stein Variational Gradient Descent ― .sh</title><meta content="Stein Variational Gradient Descent ― .sh" property=og:title><meta content="Stein Variational Gradient Descent ― .sh" name=twitter:title><meta content="SVGD approximates the target distribution via iteratively transporting particles from simple distribution." name=description><meta content="SVGD approximates the target distribution via iteratively transporting particles from simple distribution." property=og:description><meta content="SVGD approximates the target distribution via iteratively transporting particles from simple distribution." name=twitter:description><meta content=https://blog.raifthenerd.com/thumbnail.png name=twitter:image><meta content=https://blog.raifthenerd.com/thumbnail.png property=og:image><body><header aria-label="main navigation" class="navbar is-transparent" role=navigation><div class="navbar-brand container is-max-widescreen"><div class=navbar-brand><a class="navbar-item has-text-weight-bold is-uppercase" href=https://blog.raifthenerd.com> .sh </a></div></div></header><section class=section><article class="container is-max-widescreen"><div class="columns is-variable is-8-widescreen"><header class=column><h1 class="title is-2">Stein Variational Gradient Descent</h1><p class="subtitle is-6"><time datetime=2022-02-26>2022-02-26</time><hr></header></div><div class="columns is-variable is-8-widescreen"><aside class="menu column is-one-quarter is-one-third-widescreen is-hidden-touch"><p class=menu-label>Table of Contents<ul class=menu-list><li><a href=https://blog.raifthenerd.com/svgd/#preliminaries-mcmc-and-vi>Preliminaries: MCMC and VI</a><li><a href=https://blog.raifthenerd.com/svgd/#stein-s-identity-and-kernelized-stein-discrepancy>Stein's Identity and Kernelized Stein Discrepancy</a><li><a href=https://blog.raifthenerd.com/svgd/#stein-variational-gradient-descent>Stein Variational Gradient Descent</a><li><a href=https://blog.raifthenerd.com/svgd/#reference>Reference</a></ul></aside><div class="content column"><p>The main challenge of the modern Bayesian inferences is about how to handle the intractable posterior. In many models, especially in deep neural networks, the posterior distribution consists of billions of parameters in a highly nonlinear and complex relationships. Traditional algorithms such as <a href=https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo>Markov chain Monte Carlo (MCMC)</a> and <a href=https://en.wikipedia.org/wiki/Variational_Bayesian_methods>variational inference (VI)</a> are usually not suitable; the former lacks scalability and the latter lacks expressive power.<p>Stein variational gradient descent (SVGD), suggested by <a href=https://proceedings.neurips.cc/paper/2016/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf>Liu & Wang</a>, is a new algorithm for Bayesian models different from the MCMC and VI.</p><span id=continue-reading></span><p>See the <a href="https://chi-feng.github.io/mcmc-demo/app.html?algorithm=SVGD&delay=0">visualization of SVGD</a>, © Chi Feng.<h2 id=preliminaries-mcmc-and-vi>Preliminaries: MCMC and VI</h2><p>Assume that the target distribution \(p\) supported on \(\mathbb{R}^d\) is smooth. Roughly speaking, traditional Bayesian algorithms can be summarized as followed:<ul><li>MCMC samples \(x_i\) from a certain Markov chain of \(p\), and apply Monte Carlo approaches in inferences.<li>VI finds \(q\) from a well-known family of distribution \(\mathcal{Q}\) such that \(q\simeq p\), and use \(q\) in inference instead of \(p\).</ul><h2 id=stein-s-identity-and-kernelized-stein-discrepancy>Stein's Identity and Kernelized Stein Discrepancy</h2><p>Consider another smooth distribution \(q\) supported on \(\mathbb{R}^d\). Before approximating \(q\) to \(p\), we must define some <em>discrepancy</em> metric between distributions to measure the quality of approximation. As the name suggest, SVGD use <a href=https://en.wikipedia.org/wiki/Stein_discrepancy>Stein discrepancy</a> as the metric.<p>Before we move further, note that the following identity holds.<p><strong>Proposition.</strong> Let \(\phi:\mathbb{R}^d\to\mathbb{R}^d\) is smooth. Define the <em>Stein operator</em> \(\mathcal{A}_q \phi(x):=\phi(x)\nabla_x\log q(x)^\top + \nabla_x\phi(x)\). Under some mild conditions, we have \(\mathbb{E}_{x\sim q}\left[\mathcal{A}_q\phi(x)\right]=0\).<p>This implies that \(\mathbb{E}_{x\sim q}\left[\mathcal{A}_p\phi(x)\right]=0\) relates to the discrepancy between \(p\) and \(q\), which leads to the following definition.<p><strong>Definition.</strong> (Stein discrepancy) For some proper function class \(\mathcal{F}\), the <em>Stein discrepancy</em> is \[ \mathbb{S}(q, p)=\sup_{\phi\in\mathcal{F}}\left[\mathbb{E}_{x\sim q}\text{tr}\mathcal{A}_p\phi(x)\right]^2. \]<p>The supremum can be evaluated in closed form using a particular choice of \(\mathcal{F}\). For a reproducing kernel \(\kappa: \mathbb{R}^d\times\mathbb{R}^d\to\mathbb{R}\) and the corresponding RKHS \(H_\kappa\), define \(\mathcal{F}_P=\{f\in H_\kappa: \|f\|_{H_\kappa}\leq 1\}\). Then, the optimal solution has been shown to be \[ \mathbb{S}(q, p)=\|\phi^\ast\|_{H_\kappa}^2, \quad\text{where } \phi^\ast(\cdot)\propto\mathbb{E}_{x\sim q}\mathcal{A}_p\kappa(x, \cdot). \]<h2 id=stein-variational-gradient-descent>Stein Variational Gradient Descent</h2><p>SVGD approximates \(p\) using <em>samples</em> from arbitrary reference distribution \(q_0\), followed by <em>series of certain transforms</em>. In other words, the goal is find \(T_1,\dots,T_n:\mathbb{R}^d\to\mathbb{R}^d\) such that \(T_n\circ\dots\circ T_1\circ q_0 \simeq p\).<p>To make the objective solvable, consider the following types of transforms only; \[ T(x)=x+\epsilon\phi(x), \] where \(\epsilon>0\) is a perturbation magnitude and \(\phi\) is a perturbation direction. Furthermore, assume that \(T\) is smooth and invertable, thus one-to-one. Then, the following theorem holds:<p><strong>Theorem.</strong> Denote \(q_{[T]}\) as the pushforward distribution of \(q\) through \(T\). Then, \[ \nabla_{\epsilon=0}\text{KL}(q_{[T]}|p)=-\mathbb{E}_{x\sim q}\left[\text{tr}\mathcal{A}_p\phi(x)\right]. \]<p>The theorem suggests that the perturbation direction \(\phi\) determines the amount of decrease of KL divergence. Since the goal is approximate \(p\) via \(q\) and \(T\), one can apply the steepest descent method here by choosing \(\phi^\ast(\cdot)=\mathbb{E}_{x\sim q}\mathcal{A}_p\kappa(x, \cdot)\) as mentioned above.<p>In conclusion, the SVGD algorithm is stated as followed:<p>\[\begin{aligned} &\textbf{Input:}\text{ A target distribution with density function }p(x) \text{ and a set of initial particles }\{x_i^0\}_{i=1}^n\\ &\textbf{for }\text{iteration }\ell\textbf{ do}\\ &\hspace{5mm} x_i^{\ell+1}\gets x_i^\ell + \epsilon_\ell\hat\phi^\ast(x_i^\ell) \\ &\hspace{10mm}\text{where }\hat\phi^\ast(x) = \frac1n\sum_{j=1}^n\left[\kappa(x, x_j^\ell)\nabla_{x_j^\ell}\log p(x_j^\ell)+\nabla_{x_j^\ell}\kappa(x, x_j^\ell)\right]\\ &\textbf{end for}\\ &\textbf{Output:}\text{ A set of particles }\{x_i\}_{i=1}^n \text{ that approximates the target distribution} \end{aligned}\]<p>Intuitively, the update pushes the particles towards the modes of the target distribution via \(\nabla\log p\), while \(\nabla\kappa(x, x')\) prevents mode collapsing.<p>Furthermore, the algorithm can be executed more efficiently by using some techniques such as:<ul><li>Approximate \(\nabla_x\log p(x) \) from mini-batch.<li>Kernel approximation, <em>e.g.</em>, using <a href=https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf>random Fourier feature</a>.</ul><h2 id=reference>Reference</h2><ul><li>Liu & Wang (NeurIPS 2016). <a href=https://proceedings.neurips.cc/paper/2016/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf>Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</a>.<li>Liu (NeurIPS 2017). <a href=https://proceedings.neurips.cc/paper/2017/file/17ed8abedc255908be746d245e50263a-Paper.pdf>Stein Variational Gradient Descent as Gradient Flow</a></ul></div></div><div class="columns is-variable is-8-widescreen"><article class="giscus column"></article></div></article></section><footer class="footer mt-auto"><div class="container is-max-widescreen"><div class="image is-96x96 ml-auto mr-auto"><img alt="ascii kitten" src=/nyan.svg></div><div class="has-text-centered is-size-7"><small> © <a rel="author external" href=https://github.com/raifthenerd/raifthenerd>Seokjin Han</a> 2022. All contents on this website is licensed under <a rel="license external" href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA 4.0</a>. <br> <a href=/policy rel=license>Privacy Policy</a> </small></div></div></footer>